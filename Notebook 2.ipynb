{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L29nMiwqAgLG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GraviteAccidentFrance\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfUsagers = spark.read.csv(\"usagers-2023.csv\", header=True, inferSchema=True, sep=';')\n",
        "dfVehicules = spark.read.csv(\"vehicules-2023.csv\", header=True, inferSchema=True, sep=';')\n",
        "dfLieux = spark.read.csv(\"lieux-2023.csv\", header=True, inferSchema=True, sep=';')\n",
        "dfCaracteristiques = spark.read.csv(\"caract-2023.csv\", header=True, inferSchema=True, sep=';')\n"
      ],
      "metadata": {
        "id": "l_D4v6p4ApZC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfUsagers.printSchema()\n",
        "dfUsagers.show(3)\n",
        "\n",
        "dfVehicules.printSchema()\n",
        "dfCaracteristiques.printSchema()\n",
        "dfLieux.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMpIuezAAs6y",
        "outputId": "6e8cc00e-ea06-4363-a526-e9de695e54dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Num_Acc: long (nullable = true)\n",
            " |-- id_usager: string (nullable = true)\n",
            " |-- id_vehicule: string (nullable = true)\n",
            " |-- num_veh: string (nullable = true)\n",
            " |-- place: double (nullable = true)\n",
            " |-- catu: integer (nullable = true)\n",
            " |-- grav: double (nullable = true)\n",
            " |-- sexe: double (nullable = true)\n",
            " |-- an_nais: integer (nullable = true)\n",
            " |-- trajet: double (nullable = true)\n",
            " |-- secu1: double (nullable = true)\n",
            " |-- secu2: double (nullable = true)\n",
            " |-- secu3: double (nullable = true)\n",
            " |-- locp: double (nullable = true)\n",
            " |-- actp: string (nullable = true)\n",
            " |-- etatp: double (nullable = true)\n",
            "\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+\n",
            "|     Num_Acc|  id_usager|id_vehicule|num_veh|place|catu|grav|sexe|an_nais|trajet|secu1|secu2|secu3|locp|actp|etatp|\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+\n",
            "|202300000001|203 851 184|155 680 557|    A01|  1.0|   1| 4.0| 1.0|   1978|   5.0|  2.0|  0.0| -1.0|-1.0|  -1| -1.0|\n",
            "|202300000002|203 851 182|155 680 556|    A01|  1.0|   1| 1.0| 2.0|   1997|   9.0|  1.0|  0.0| -1.0|-1.0|  -1| -1.0|\n",
            "|202300000002|203 851 183|155 680 556|    A01| 10.0|   3| 3.0| 1.0|   1997|   9.0|  0.0| -1.0| -1.0| 2.0|   3|  1.0|\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "root\n",
            " |-- Num_Acc: long (nullable = true)\n",
            " |-- id_vehicule: string (nullable = true)\n",
            " |-- num_veh: string (nullable = true)\n",
            " |-- senc: double (nullable = true)\n",
            " |-- catv: double (nullable = true)\n",
            " |-- obs: double (nullable = true)\n",
            " |-- obsm: double (nullable = true)\n",
            " |-- choc: double (nullable = true)\n",
            " |-- manv: double (nullable = true)\n",
            " |-- motor: double (nullable = true)\n",
            " |-- occutc: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Num_Acc: long (nullable = true)\n",
            " |-- jour: integer (nullable = true)\n",
            " |-- mois: integer (nullable = true)\n",
            " |-- an: integer (nullable = true)\n",
            " |-- hrmn: timestamp (nullable = true)\n",
            " |-- lum: double (nullable = true)\n",
            " |-- dep: string (nullable = true)\n",
            " |-- com: string (nullable = true)\n",
            " |-- agg: integer (nullable = true)\n",
            " |-- int: double (nullable = true)\n",
            " |-- atm: double (nullable = true)\n",
            " |-- col: double (nullable = true)\n",
            " |-- adr: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- long: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Num_Acc: long (nullable = true)\n",
            " |-- catr: integer (nullable = true)\n",
            " |-- voie: string (nullable = true)\n",
            " |-- v1: double (nullable = true)\n",
            " |-- v2: string (nullable = true)\n",
            " |-- circ: double (nullable = true)\n",
            " |-- nbv: string (nullable = true)\n",
            " |-- vosp: double (nullable = true)\n",
            " |-- prof: double (nullable = true)\n",
            " |-- pr: string (nullable = true)\n",
            " |-- pr1: string (nullable = true)\n",
            " |-- plan: double (nullable = true)\n",
            " |-- lartpc: string (nullable = true)\n",
            " |-- larrout: string (nullable = true)\n",
            " |-- surf: double (nullable = true)\n",
            " |-- infra: double (nullable = true)\n",
            " |-- situ: double (nullable = true)\n",
            " |-- vma: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Jointure usagers ↔ véhicules\n",
        "df_uv = dfUsagers.join(dfVehicules, on=[\"Num_Acc\", \"id_vehicule\"], how=\"left\")\n",
        "\n",
        "# 2. Ajout des caractéristiques d'accident\n",
        "df_uvc = df_uv.join(dfCaracteristiques, on=\"Num_Acc\", how=\"left\")\n",
        "\n",
        "# 3. Ajout des informations du lieu\n",
        "df_final = df_uvc.join(dfLieux, on=\"Num_Acc\", how=\"left\")\n",
        "\n",
        "# Affichage pour vérification\n",
        "df_final.printSchema()\n",
        "df_final.show(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCL7GYYBBBT5",
        "outputId": "69079b82-85fb-4762-f944-e2efd25417e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Num_Acc: long (nullable = true)\n",
            " |-- id_vehicule: string (nullable = true)\n",
            " |-- id_usager: string (nullable = true)\n",
            " |-- num_veh: string (nullable = true)\n",
            " |-- place: double (nullable = true)\n",
            " |-- catu: integer (nullable = true)\n",
            " |-- grav: double (nullable = true)\n",
            " |-- sexe: double (nullable = true)\n",
            " |-- an_nais: integer (nullable = true)\n",
            " |-- trajet: double (nullable = true)\n",
            " |-- secu1: double (nullable = true)\n",
            " |-- secu2: double (nullable = true)\n",
            " |-- secu3: double (nullable = true)\n",
            " |-- locp: double (nullable = true)\n",
            " |-- actp: string (nullable = true)\n",
            " |-- etatp: double (nullable = true)\n",
            " |-- num_veh: string (nullable = true)\n",
            " |-- senc: double (nullable = true)\n",
            " |-- catv: double (nullable = true)\n",
            " |-- obs: double (nullable = true)\n",
            " |-- obsm: double (nullable = true)\n",
            " |-- choc: double (nullable = true)\n",
            " |-- manv: double (nullable = true)\n",
            " |-- motor: double (nullable = true)\n",
            " |-- occutc: integer (nullable = true)\n",
            " |-- jour: integer (nullable = true)\n",
            " |-- mois: integer (nullable = true)\n",
            " |-- an: integer (nullable = true)\n",
            " |-- hrmn: timestamp (nullable = true)\n",
            " |-- lum: double (nullable = true)\n",
            " |-- dep: string (nullable = true)\n",
            " |-- com: string (nullable = true)\n",
            " |-- agg: integer (nullable = true)\n",
            " |-- int: double (nullable = true)\n",
            " |-- atm: double (nullable = true)\n",
            " |-- col: double (nullable = true)\n",
            " |-- adr: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- long: string (nullable = true)\n",
            " |-- catr: integer (nullable = true)\n",
            " |-- voie: string (nullable = true)\n",
            " |-- v1: double (nullable = true)\n",
            " |-- v2: string (nullable = true)\n",
            " |-- circ: double (nullable = true)\n",
            " |-- nbv: string (nullable = true)\n",
            " |-- vosp: double (nullable = true)\n",
            " |-- prof: double (nullable = true)\n",
            " |-- pr: string (nullable = true)\n",
            " |-- pr1: string (nullable = true)\n",
            " |-- plan: double (nullable = true)\n",
            " |-- lartpc: string (nullable = true)\n",
            " |-- larrout: string (nullable = true)\n",
            " |-- surf: double (nullable = true)\n",
            " |-- infra: double (nullable = true)\n",
            " |-- situ: double (nullable = true)\n",
            " |-- vma: double (nullable = true)\n",
            "\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+-------+----+----+---+----+----+----+-----+------+----+----+----+-------------------+---+---+-----+---+---+---+---+---------------+-----------+----------+----+-------------------+---+---+----+---+----+----+---+---+----+------+-------+----+-----+----+----+\n",
            "|     Num_Acc|id_vehicule|  id_usager|num_veh|place|catu|grav|sexe|an_nais|trajet|secu1|secu2|secu3|locp|actp|etatp|num_veh|senc|catv|obs|obsm|choc|manv|motor|occutc|jour|mois|  an|               hrmn|lum|dep|  com|agg|int|atm|col|            adr|        lat|      long|catr|               voie| v1| v2|circ|nbv|vosp|prof| pr|pr1|plan|lartpc|larrout|surf|infra|situ| vma|\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+-------+----+----+---+----+----+----+-----+------+----+----+----+-------------------+---+---+-----+---+---+---+---+---------------+-----------+----------+----+-------------------+---+---+----+---+----+----+---+---+----+------+-------+----+-----+----+----+\n",
            "|202300000001|155 680 557|203 851 184|    A01|  1.0|   1| 4.0| 1.0|   1978|   5.0|  2.0|  0.0| -1.0|-1.0|  -1| -1.0|    A01| 1.0|30.0|0.0| 0.0| 5.0| 1.0|  1.0|  NULL|   7|   5|2023|2025-06-25 06:00:00|1.0| 75|75101|  2|4.0|2.0|7.0|  RUE DE RIVOLI|48,86638600|2,32347100|   4|RUE SAINT FLORENTIN|0.0|N/A| 1.0|  1| 0.0| 1.0| -1| -1| 1.0|  NULL|     -1| 2.0|  0.0| 1.0|30.0|\n",
            "|202300000001|155 680 557|203 851 184|    A01|  1.0|   1| 4.0| 1.0|   1978|   5.0|  2.0|  0.0| -1.0|-1.0|  -1| -1.0|    A01| 1.0|30.0|0.0| 0.0| 5.0| 1.0|  1.0|  NULL|   7|   5|2023|2025-06-25 06:00:00|1.0| 75|75101|  2|4.0|2.0|7.0|  RUE DE RIVOLI|48,86638600|2,32347100|   4|      RUE DE RIVOLI|0.0|N/A| 1.0|  2| 0.0| 1.0| -1| -1| 1.0|  NULL|     -1| 2.0|  0.0| 1.0|30.0|\n",
            "|202300000002|155 680 556|203 851 182|    A01|  1.0|   1| 1.0| 2.0|   1997|   9.0|  1.0|  0.0| -1.0|-1.0|  -1| -1.0|    A01| 2.0| 7.0|0.0| 1.0| 1.0| 1.0|  1.0|  NULL|   7|   5|2023|2025-06-25 05:30:00|5.0| 94|94080|  2|1.0|3.0|6.0|Avenue de Paris|48,84547782|2,42868146|   3|                120|0.0|N/A| 2.0|  3| 2.0| 1.0| -1| -1| 1.0|  NULL|     -1| 2.0|  0.0| 1.0|50.0|\n",
            "+------------+-----------+-----------+-------+-----+----+----+----+-------+------+-----+-----+-----+----+----+-----+-------+----+----+---+----+----+----+-----+------+----+----+----+-------------------+---+---+-----+---+---+---+---+---------------+-----------+----------+----+-------------------+---+---+----+---+----+----+---+---+----+------+-------+----+-----+----+----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Supprimer doublon de colonnes\n",
        "df_clean = df_final.drop(\"num_veh\")  # une des deux versions\n",
        "\n",
        "# Grave : Tué (2) ou hospitalisé (3)\n",
        "df_clean = df_clean.withColumn(\n",
        "    \"target\", when(col(\"grav\").isin(2.0, 3.0, 4.0), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "\n",
        "# Optionnel : filtrer les lignes où certaines valeurs clés sont manquantes ou invalides\n",
        "df_clean = df_clean.filter(\n",
        "    (col(\"grav\").isNotNull()) & (col(\"grav\") != -1) &\n",
        "    (col(\"sexe\").isNotNull()) & (col(\"sexe\") != -1) &\n",
        "    (col(\"catv\").isNotNull()) &\n",
        "    (col(\"atm\").isNotNull()) &\n",
        "    (col(\"lum\").isNotNull())\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZvvKCsYWBx74"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, lit\n",
        "\n",
        "# Création de la colonne age (en supposant les données de 2023)\n",
        "df_clean = df_clean.withColumn(\"age\", lit(2023) - col(\"an_nais\"))\n"
      ],
      "metadata": {
        "id": "NHaj_FiQCpkK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifions la distribution des classes\n",
        "df_clean.groupBy(\"target\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqV5py-wCPzV",
        "outputId": "cc5fbae6-1a4a-4b72-8782-d5490fafc9f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     1|92665|\n",
            "|     0|67739|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = [\n",
        "    \"age\", \"place\", \"catv\", \"atm\", \"lum\", \"circ\",\n",
        "    \"choc\", \"manv\", \"motor\", \"vma\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "3-ex2Wv5CwpH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [\n",
        "    \"sexe\", \"catu\", \"trajet\", \"secu1\", \"agg\", \"dep\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "1AC82SNnCw9g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Indexeurs\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\")\n",
        "            for col in categorical_cols]\n",
        "\n",
        "# Colonnes finales à assembler\n",
        "final_features = [col + \"_indexed\" for col in categorical_cols] + numeric_cols\n",
        "\n",
        "# Assembler toutes les features dans une seule colonne \"features\"\n",
        "assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
        "\n",
        "# Pipeline encodage + assemblage\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "\n",
        "# Appliquer le pipeline\n",
        "df_model = pipeline.fit(df_clean).transform(df_clean)\n",
        "\n",
        "# Vérif\n",
        "df_model.select(\"features\", \"target\").show(3, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_kt1zrZC3gZ",
        "outputId": "80f593df-9f85-4706-bc8d-4d5a388bbadc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+------+\n",
            "|features                                                            |target|\n",
            "+--------------------------------------------------------------------+------+\n",
            "|[0.0,0.0,0.0,1.0,0.0,0.0,45.0,1.0,30.0,2.0,1.0,1.0,5.0,1.0,1.0,30.0]|1     |\n",
            "|[0.0,0.0,0.0,1.0,0.0,0.0,45.0,1.0,30.0,2.0,1.0,1.0,5.0,1.0,1.0,30.0]|1     |\n",
            "|[1.0,0.0,4.0,0.0,0.0,4.0,26.0,1.0,7.0,3.0,5.0,2.0,1.0,1.0,1.0,50.0] |0     |\n",
            "+--------------------------------------------------------------------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_model_clean = df_model.dropna(subset=[\"age\"])"
      ],
      "metadata": {
        "id": "aEB9aos2DWiH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import col, sum\n",
        "\n",
        "# # Combiner toutes les colonnes utilisées dans le VectorAssembler\n",
        "# all_features = [col + \"_indexed\" for col in categorical_cols] + numeric_cols\n",
        "\n",
        "# # Calculer le nombre de NULLs par colonne\n",
        "# null_counts = df_model_clean.select([\n",
        "#     sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "#     for c in all_features\n",
        "# ])\n",
        "\n",
        "# # Afficher les résultats\n",
        "# null_counts.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "YvWsEl-4D6RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.classification import GBTClassifier\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# # Split train/test\n",
        "# train_data, test_data = df_model_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# # Modèle GBT\n",
        "# gbt = GBTClassifier(labelCol=\"target\", featuresCol=\"features\", maxIter=20)\n",
        "\n",
        "# # Entraînement\n",
        "# model = gbt.fit(train_data)\n",
        "\n",
        "# # Prédictions\n",
        "# predictions = model.transform(test_data)\n",
        "\n",
        "# # Évaluation\n",
        "# evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "# auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# print(\"AUC (GBT Classifier) :\", round(auc, 4))\n"
      ],
      "metadata": {
        "id": "2LWXn85EDIJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "categorical_cols = [\"sexe\", \"catu\", \"trajet\", \"secu1\", \"agg\", \"dep\"]\n",
        "numeric_cols = [\"age\", \"place\", \"catv\", \"atm\", \"lum\", \"circ\", \"choc\", \"manv\", \"motor\", \"vma\"]\n",
        "\n",
        "# Encodage avec protection contre valeurs inconnues\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "# VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[col + \"_indexed\" for col in categorical_cols] + numeric_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"  # encore plus safe\n",
        ")\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "df_model = pipeline.fit(df_clean).transform(df_clean)\n"
      ],
      "metadata": {
        "id": "8RDAMBQXFSGS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model_clean = df_model.dropna(subset=[\"features\"])"
      ],
      "metadata": {
        "id": "RD1N-JBEFabx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "train_data, test_data = df_model_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "gbt = GBTClassifier(\n",
        "    labelCol=\"target\",\n",
        "    featuresCol=\"features\",\n",
        "    maxIter=20,\n",
        "    maxBins=256  # très important pour éviter l'erreur liée à dep\n",
        ")\n",
        "gbt_model = gbt.fit(train_data)\n",
        "\n",
        "predictions = gbt_model.transform(test_data)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"✅ AUC (GBT Classifier) :\", round(auc, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXUskg3_FeCE",
        "outputId": "4f1e29f2-55e8-4ec9-a324-4918c3f6d6f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AUC (GBT Classifier) : 0.847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Arrondir la prédiction à 0 ou 1\n",
        "predictions_rounded = predictions.withColumn(\"prediction_bin\", col(\"prediction\").cast(\"int\"))\n",
        "\n",
        "# Matrice de confusion\n",
        "confusion_matrix = predictions_rounded.groupBy(\"target\", \"prediction_bin\").count().orderBy(\"target\", \"prediction_bin\")\n",
        "confusion_matrix.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgXg4BdeGbWK",
        "outputId": "e1865e4d-a05a-497c-8d61-487ac0225325"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-----+\n",
            "|target|prediction_bin|count|\n",
            "+------+--------------+-----+\n",
            "|     0|             0| 9956|\n",
            "|     0|             1| 3572|\n",
            "|     1|             0| 4047|\n",
            "|     1|             1|14417|\n",
            "+------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Split des données\n",
        "train_data, test_data = df_model_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Modèle Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    labelCol=\"target\",\n",
        "    featuresCol=\"features\",\n",
        "    numTrees=100,         # tu peux ajuster\n",
        "    maxBins=256,\n",
        "    maxDepth=5,           # profondeur max de l’arbre\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Entraînement\n",
        "rf_model = rf.fit(train_data)\n",
        "\n",
        "# Prédiction\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "\n",
        "# Évaluation AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "auc_rf = evaluator.evaluate(rf_predictions)\n",
        "\n",
        "print(\"✅ AUC (Random Forest) :\", round(auc_rf, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaNW8oRyOgxe",
        "outputId": "13d25fe8-ea04-4916-f522-cf46b411e1ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AUC (Random Forest) : 0.8173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Créer une colonne 'prediction_bin' au cas où ce n’est pas encore un entier\n",
        "predictions = predictions.withColumn(\"prediction_bin\", col(\"prediction\").cast(\"int\"))\n",
        "\n",
        "# Matrice de confusion\n",
        "confusion_matrix = predictions.groupBy(\"target\", \"prediction_bin\").count().orderBy(\"target\", \"prediction_bin\")\n",
        "confusion_matrix.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHydslkrQPg7",
        "outputId": "c6f882c6-3757-4f9d-8b4f-ba98388506a0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-----+\n",
            "|target|prediction_bin|count|\n",
            "+------+--------------+-----+\n",
            "|     0|             0| 9956|\n",
            "|     0|             1| 3572|\n",
            "|     1|             0| 4047|\n",
            "|     1|             1|14417|\n",
            "+------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Étape 1 : Séparation des données\n",
        "train_data, test_data = df_model_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Étape 2 : Modèle\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"target\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # 0 = L2 régularisation (Ridge)\n",
        ")\n",
        "\n",
        "# Étape 3 : Entraînement\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Étape 4 : Prédiction\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Étape 5 : AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "auc_lr = evaluator.evaluate(lr_predictions)\n",
        "\n",
        "print(\"✅ AUC (Logistic Regression) :\", round(auc_lr, 4))\n",
        "\n",
        "# Étape 6 : Matrice de confusion\n",
        "lr_predictions = lr_predictions.withColumn(\"prediction_bin\", col(\"prediction\").cast(\"int\"))\n",
        "lr_predictions.groupBy(\"target\", \"prediction_bin\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yE402VaQsGq",
        "outputId": "b42936fc-b786-4523-9358-0a876a3a67bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AUC (Logistic Regression) : 0.7659\n",
            "+------+--------------+-----+\n",
            "|target|prediction_bin|count|\n",
            "+------+--------------+-----+\n",
            "|     1|             0| 4649|\n",
            "|     1|             1|13815|\n",
            "|     0|             0| 8657|\n",
            "|     0|             1| 4871|\n",
            "+------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "def save_best_model(test_data, export_path=\"models/best_model\"):\n",
        "    \"\"\"\n",
        "    Sauvegarde le meilleur modèle (selon AUC) parmi gbt_model, rf_model et lr_model.\n",
        "\n",
        "    Args:\n",
        "        test_data (DataFrame): le DataFrame de test.\n",
        "        export_path (str): chemin de sauvegarde du meilleur modèle.\n",
        "    \"\"\"\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "\n",
        "    models = [\n",
        "        (\"GBT Classifier\", gbt_model),\n",
        "        (\"Random Forest\", rf_model),\n",
        "        (\"Logistic Regression\", lr_model)\n",
        "    ]\n",
        "\n",
        "    best_auc = 0.0\n",
        "    best_model = None\n",
        "    best_name = \"\"\n",
        "\n",
        "    for name, model in models:\n",
        "        predictions = model.transform(test_data)\n",
        "        auc = evaluator.evaluate(predictions)\n",
        "        print(f\"AUC ({name}) : {round(auc, 4)}\")\n",
        "\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_model = model\n",
        "            best_name = name\n",
        "\n",
        "    if best_model:\n",
        "        print(f\"\\n✅ Meilleur modèle : {best_name} avec AUC = {round(best_auc, 4)}\")\n",
        "        best_model.write().overwrite().save(export_path)\n",
        "        print(f\"💾 Modèle sauvegardé dans : {export_path}\")\n",
        "    else:\n",
        "        print(\"❌ Aucun modèle sauvegardé.\")\n"
      ],
      "metadata": {
        "id": "LKQef53sSiCo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_model(test_data, \"models/best_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaRT2A3BSl9z",
        "outputId": "f8a2e89d-bf2b-413d-f080-ea57bfd34cd3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC (GBT Classifier) : 0.847\n",
            "AUC (Random Forest) : 0.8173\n",
            "AUC (Logistic Regression) : 0.7659\n",
            "\n",
            "✅ Meilleur modèle : GBT Classifier avec AUC = 0.847\n",
            "💾 Modèle sauvegardé dans : models/best_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import col, when\n",
        "# from pyspark.ml.classification import RandomForestClassifier\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# # --- Étape 1 : Calculer les poids de classe ---\n",
        "# n_total = df_model_clean.count()\n",
        "# n_pos = df_model_clean.filter(col(\"target\") == 1).count()\n",
        "# n_neg = df_model_clean.filter(col(\"target\") == 0).count()\n",
        "\n",
        "# # Ratio pour renforcer les exemples positifs (graves)\n",
        "# balancing_ratio = n_neg / n_pos\n",
        "\n",
        "# # Ajouter la colonne de poids\n",
        "# df_weighted = df_model_clean.withColumn(\n",
        "#     \"classWeightCol\",\n",
        "#     when(col(\"target\") == 1, balancing_ratio).otherwise(1.0)\n",
        "# )\n",
        "\n",
        "# # --- Étape 2 : Split des données ---\n",
        "# train_data, test_data = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# # --- Étape 3 : Définir et entraîner le modèle Random Forest ---\n",
        "# rf = RandomForestClassifier(\n",
        "#     labelCol=\"target\",\n",
        "#     featuresCol=\"features\",\n",
        "#     weightCol=\"classWeightCol\",\n",
        "#     numTrees=100,\n",
        "#     maxBins=256\n",
        "# )\n",
        "\n",
        "# rf_model = rf.fit(train_data)\n",
        "\n",
        "# # --- Étape 4 : Prédictions et évaluation ---\n",
        "# predictions = rf_model.transform(test_data)\n",
        "\n",
        "# evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "# auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# print(\"🌲 AUC (Random Forest Classifier) :\", round(auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3TEYjYuIm1f",
        "outputId": "07d6422a-891a-4445-f8ec-50d180292477"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌲 AUC (Random Forest Classifier) : 0.8021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import col\n",
        "\n",
        "# # Arrondir la prédiction (float) en binaire\n",
        "# predictions_rounded = predictions.withColumn(\"prediction_bin\", col(\"prediction\").cast(\"int\"))\n",
        "\n",
        "# # Matrice de confusion\n",
        "# confusion_matrix = predictions_rounded.groupBy(\"target\", \"prediction_bin\").count().orderBy(\"target\", \"prediction_bin\")\n",
        "# confusion_matrix.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTvjnKhHJraj",
        "outputId": "912318d9-e530-4b3e-beb9-3dd0f89b4ddd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-----+\n",
            "|target|prediction_bin|count|\n",
            "+------+--------------+-----+\n",
            "|     0|             0|18138|\n",
            "|     0|             1| 8302|\n",
            "|     1|             0| 1258|\n",
            "|     1|             1| 4294|\n",
            "+------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.classification import LogisticRegression\n",
        "# from pyspark.sql.functions import col, when\n",
        "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# # --- Étape 1 : Ajouter classWeightCol ---\n",
        "# n_total = df_model_clean.count()\n",
        "# n_pos = df_model_clean.filter(col(\"target\") == 1).count()\n",
        "# n_neg = df_model_clean.filter(col(\"target\") == 0).count()\n",
        "\n",
        "# balancing_ratio = n_neg / n_pos\n",
        "\n",
        "# df_weighted = df_model_clean.withColumn(\n",
        "#     \"classWeightCol\",\n",
        "#     when(col(\"target\") == 1, balancing_ratio).otherwise(1.0)\n",
        "# )\n",
        "\n",
        "# # --- Étape 2 : Split train/test ---\n",
        "# train_data, test_data = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# # --- Étape 3 : Définir et entraîner le modèle Logistic Regression ---\n",
        "# lr = LogisticRegression(\n",
        "#     labelCol=\"target\",\n",
        "#     featuresCol=\"features\",\n",
        "#     weightCol=\"classWeightCol\",  # important pour déséquilibre\n",
        "#     maxIter=100,\n",
        "#     regParam=0.0,       # pas de régularisation pour commencer\n",
        "#     elasticNetParam=0.0\n",
        "# )\n",
        "\n",
        "# lr_model = lr.fit(train_data)\n",
        "\n",
        "# # --- Étape 4 : Prédictions et évaluation ---\n",
        "# predictions = lr_model.transform(test_data)\n",
        "\n",
        "# evaluator = BinaryClassificationEvaluator(labelCol=\"target\", metricName=\"areaUnderROC\")\n",
        "# auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# print(\"📈 AUC (Logistic Regression) :\", round(auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuohVag6KVAm",
        "outputId": "fea730ac-8342-422e-b2ec-138e8e9eba70"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 AUC (Logistic Regression) : 0.7574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Matrice de confusion\n",
        "# predictions_rounded = predictions.withColumn(\"prediction_bin\", col(\"prediction\").cast(\"int\"))\n",
        "# predictions_rounded.groupBy(\"target\", \"prediction_bin\").count().orderBy(\"target\", \"prediction_bin\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh5QzWRsK5eD",
        "outputId": "f3de100b-a99f-4367-d7e4-c58bf1cf9670"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-----+\n",
            "|target|prediction_bin|count|\n",
            "+------+--------------+-----+\n",
            "|     0|             0|18346|\n",
            "|     0|             1| 8094|\n",
            "|     1|             0| 1795|\n",
            "|     1|             1| 3757|\n",
            "+------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.save(\"rf_model_accidents\")"
      ],
      "metadata": {
        "id": "VRjZS2OxO2Jb"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}